{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 0, CNN\n",
    "This notebook solves the problem of multiclass classification. It will classify 3 different classes of input images - squares, triangles and lines. The Given, Find, Structure of this submission is seperated into seperate tasks that you're introduced to throughout the Jupyter Notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: random 5x5 integer matrix\n",
    "# 3 classes: Triangle (0),  Line (1), Square (2)\n",
    "# The shape of the input matrix is (1, 5, 5) and not (5, 5) because we can later generalize to images with more than one channel\n",
    "input_triangle = np.array([[\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0]\n",
    "]])\n",
    "input_line = np.array([[\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0]\n",
    "]])\n",
    "input_square = np.array([[\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0]\n",
    "]])\n",
    "\n",
    "TRIANGLE = np.array([1., 0., 0.])\n",
    "LINE = np.array([0., 1., 0.])\n",
    "SQUARE = np.array([0., 0., 1.])\n",
    "images = np.array([input_triangle, input_line, input_square])\n",
    "labels = np.array([TRIANGLE, LINE, SQUARE]).reshape(3, 3, 1)\n",
    "\n",
    "(x_train, y_train) = (np.array(images), np.array(labels))\n",
    "(x_test, y_test) = (np.array(images), np.array(labels))\n",
    "\n",
    "utils.display_images([(images[0, 0], \"Input image example\")], figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/analytics-vidhya/2d-convolution-using-python-numpy-43442ff5f381\n",
    "# https://github.com/macbuse/macbuse.github.io/blob/master/PROG/convolve.py\n",
    "def convolve2D(image: np.ndarray, kernel: np.ndarray, mode = \"valid\", correlation = False):\n",
    "    # Cross correlation or Convolution depends on the orientation of the kernel\n",
    "    kernel = kernel if correlation else np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    # Gather shapes of kernel + image + padding\n",
    "    x_kernel_size, y_kernel_size = kernel.shape\n",
    "    x_image_size, y_image_size = image.shape[0:2]\n",
    "\n",
    "    if mode == \"valid\":\n",
    "        # Ouput matrix size of a full convolution\n",
    "        x_output = int(x_image_size - x_kernel_size + 1)\n",
    "        y_output = int(y_image_size - y_kernel_size + 1)\n",
    "        output = np.zeros((x_output, y_output))\n",
    "\n",
    "        for y in range(0, y_image_size - y_kernel_size + 1):\n",
    "            for x in range(0, x_image_size - x_kernel_size + 1):\n",
    "                output[x, y] = (kernel * image[x: x + x_kernel_size, y: y + y_kernel_size]).sum()\n",
    "    \n",
    "    elif mode == \"full\":\n",
    "        # Output matrix size of a valid convolution\n",
    "        x_output = int((x_image_size + x_kernel_size - 1))\n",
    "        y_output = int((y_image_size + y_kernel_size - 1))\n",
    "        image_padded = np.pad(image, x_kernel_size - 1, mode='constant', constant_values=(0))\n",
    "        output = np.zeros((x_output, y_output))\n",
    "\n",
    "        for x in range(0, output.shape[0]):\n",
    "            for y in range(0, output.shape[1]):\n",
    "                output[x, y] = (kernel * image_padded[x: x + x_kernel_size, y: y + y_kernel_size]).sum()\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.0\n",
    "\n",
    "### Given\n",
    "- A dataset containing 3 different classes of images\n",
    "\n",
    "### Find\n",
    "- Apply the convolve2D-function to one of the input images with a specified kernel.\n",
    "- Apply the same convolve2D-function, but with the optional parameter `correlation = True`\n",
    "- Visualize the images leveraging `utils.display_images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x2 filter\n",
    "kernel = np.array([[1, 0], [1, 0]])\n",
    "\n",
    "# 3x3 filter\n",
    "kernel = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\n",
    "\n",
    "convoluted_output = # TODO: Use the convolve2D function to convolve the images with a kernel\n",
    "utils.display_images([(images[0, 0], \"Input image\"), (kernel, \"Kernel\"), (convoluted_output, \"Convoluted output\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1\n",
    "\n",
    "### Given\n",
    "- A skeleton for a `Layer`-class, as found in the code-block beneath\n",
    "\n",
    "### Find\n",
    "- Implement the `forward`- and `backward`-method in the `Dense` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # TODO: return output\n",
    "        pass\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # TODO: update params and return input gradient\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2\n",
    "\n",
    "### Given\n",
    "- A skeleton of a `Activation`-class\n",
    "\n",
    "### Find\n",
    "- Implement the `forward`-method in the `Sigmoid`-class\n",
    "- Implement the `forward`-method in the `Softmax`-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            # TODO: return output\n",
    "            pass\n",
    "        \n",
    "        def sigmoid_prime(x):\n",
    "            s = sigmoid(x)\n",
    "            return s * (1 - s)\n",
    "            \n",
    "        super().__init__(sigmoid, sigmoid_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        tanh = lambda x: np.tanh(x)\n",
    "        tanh_prime = lambda x: 1 - np.tanh(x)**2\n",
    "        super().__init__(tanh, tanh_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, input):\n",
    "        # TODO: return output\n",
    "        pass\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        n = np.size(self.output)\n",
    "        tmp = np.tile(self.output, n)\n",
    "        return np.dot(tmp * (np.identity(n) - np.transpose(tmp)), output_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3\n",
    "\n",
    "### Given\n",
    "- A skeleton of a `Layer`- and `Convolutional`-class\n",
    "\n",
    "### Find\n",
    "- Implement the `forward`-method in the `Convolutional`-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(Layer):\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = depth # number of kernels\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size) # multiple 3D kernels\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        # TODO: Use the convolve2D function to convolve the input with the kernels. Return the convolved output\n",
    "        pass\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        kernels_gradient = np.zeros(self.kernels_shape)\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                kernels_gradient[i, j] = convolve2D(self.input[j], output_gradient[i], \"valid\", correlation = True)\n",
    "                input_gradient[j] += convolve2D(output_gradient[i], self.kernels[i, j], \"full\")\n",
    "        \n",
    "        self.kernels -= learning_rate * kernels_gradient\n",
    "        self.biases -= learning_rate * output_gradient\n",
    "        return input_gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxpool(Layer):\n",
    "    def __init__(self, input_size, output_size, k=2, stride=1):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.k = k\n",
    "        self.stride = stride\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        h_out = (input[0].shape[0] - self.k) // self.stride + 1\n",
    "        w_out = (input[0].shape[1] - self.k) // self.stride + 1\n",
    "        \n",
    "        self.output = np.zeros((input.shape[0], h_out, w_out))\n",
    "        self.output_size = self.output.shape\n",
    "        self.idx = []\n",
    "\n",
    "        for channel in range(input.shape[0]):\n",
    "            for j in range(0, h_out):\n",
    "                for i in range(0, w_out):\n",
    "                    block = input[channel][j:j+self.k, i:i+self.k]\n",
    "                    self.output[channel][j, i] = np.max(block)\n",
    "                    index = np.add(np.unravel_index(np.argmax(block), block.shape), (j, i))\n",
    "                    self.idx.append(index)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        d = np.zeros(self.input_size)\n",
    "        for channel in range(self.output_size[0]):\n",
    "            for idx, value in zip(self.idx, output_gradient.flatten()):\n",
    "                i, j = idx[0], idx[1]\n",
    "                d[channel][i, j] = value\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(Layer):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return np.reshape(input, self.output_shape)\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.5\n",
    "\n",
    "### Loss functions\n",
    "There are a multitude of loss functions, each for its own purpose. I'll later provide 3 examples, 2 approaches to solve binary classification - one with only a FCNN, another with a CNN with binary classification. Last, a CNN with multiclass classification. Each of the different examples have a fitting loss function.\n",
    "\n",
    "Binary classification: MSE or Binary cross entropy\n",
    "Multiclass classification: Cross entropy\n",
    "\n",
    "### Given\n",
    "- `y_true` and `y_pred` as input. Both are np.ndarray with shape (3, 1)\n",
    "\n",
    "### Find\n",
    "- Implement a function for `cross_entropy(y_true, y_pred)` which returns the cross_entropy between the predicted and true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_prime(y_true, y_pred):\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    # TODO: return the cross_entropy loss\n",
    "    pass\n",
    "\n",
    "def cross_entropy_prime(y_true, y_pred):\n",
    "    # Make sure not to divide by 0 by adding a small value to y_pred\n",
    "    return - y_true / (y_pred + 10**-100)\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification Problem\n",
    "\n",
    "### Given\n",
    "- Training- and test-data in `(x_train, y_train)` and `(x_test, y_test)`\n",
    "- The implemented classes `Convolutional`, `Sigmoid`, `Reshape` and `Softmax`\n",
    "- A CNN with the following architecture and cross entropy loss function\n",
    "\n",
    "|     Layer     | Output Shape | Kernel Size | Stride | Padding | Activation function |\n",
    "|:-------------:|:------------:|:-----------:|:------:|:-------:|---------------------|\n",
    "| Input         |   (1, 5, 5)  |      -      |    -   |    -    |          -          |\n",
    "| Convolutional |   (5, 4, 4)  |     2x2     |    1   |    0    |       Sigmoid       |\n",
    "| Maxpool       |   (5, 3, 3)  |     2x2     |    1   |    0    |          -          |\n",
    "| Flatten       |    (45, 1)   |      -      |    -   |    -    |          -          |\n",
    "| Dense         |   (45, 10)   |      -      |    -   |    -    |       Sigmoid       |\n",
    "| Dense         |    (10, 3)   |      -      |    -   |    -    |       Softmax       |\n",
    "\n",
    "\n",
    "### Find\n",
    "- The intermediate kernels, gradients and outputs during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "EPOCHS = 301\n",
    "VISUALIZE_INTERVAL = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    Convolutional((1, 5, 5), 2, 5),\n",
    "    Sigmoid(),\n",
    "    Maxpool((5, 4, 4), (5, 3, 3), k = 2, stride = 1),\n",
    "    Reshape((5, 3, 3), (5 * 3 * 3, 1)),\n",
    "    Dense(5 * 3 * 3, 10),\n",
    "    Sigmoid(),\n",
    "    Dense(10, 3),\n",
    "    Softmax()\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    error = 0\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        # Forward pass\n",
    "        output = x\n",
    "\n",
    "        for layer in network:\n",
    "            temp_output = output\n",
    "            output = # TODO: Calculate the output of the layer\n",
    "\n",
    "            # TODO: Visualize how the kernel convolutes the input image\n",
    "        \n",
    "        # Calculate error\n",
    "        error += cross_entropy(y, output)\n",
    "\n",
    "        # Backward pass\n",
    "        grad = cross_entropy_prime(y, output)\n",
    "        \n",
    "        for layer in reversed(network):\n",
    "            grad = # TODO: Calculate the gradient of the loss function with respect to the output of the layer\n",
    "        \n",
    "    error /= len(x_train)\n",
    "    if (epoch % VISUALIZE_INTERVAL == 0):\n",
    "        print(f\"Epoch {epoch}, error: {error}\")\n",
    "\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = x\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    print(f\"Predicted: {np.argmax(output)}, actual: {np.argmax(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*This part can be skipped, but I've chosen to include it to showcase some extra possibilities of a framework like this*\n",
    "\n",
    "# Extra problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR-problem using a FCNN\n",
    "Example of how we can use the network to train a simple, FCNN which solves the XOR-problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape([[0,0], [0,1], [1,0], [1,1]], (4, 2, 1))\n",
    "Y = np.reshape([[0], [1], [1], [0]], (4, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    Dense(2, 3),\n",
    "    Tanh(),\n",
    "    Dense(3, 1),\n",
    "    Tanh()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    error = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        # Forward pass\n",
    "        output = x\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        # Calculate error\n",
    "        error += mse(y, output)\n",
    "\n",
    "        # Backward pass\n",
    "        grad = mse_prime(y, output)\n",
    "        \n",
    "        for layer in reversed(network):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "        \n",
    "    error /= len(X)\n",
    "    if (epoch % VISUALIZE_INTERVAL == 0):\n",
    "        print(f\"Epoch {epoch}, error: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Problem\n",
    "\n",
    "Example of how the same network can be used to solve the MNIST problem. In order to reduce the complexity and training time, the training data is reduced from 10 classes to 2, and we only train on 100 of the images. The training still takes quite long time due to the unoptimized implementations of e.g. convolve2D, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype('float32') / 255\n",
    "    y = np_utils.to_categorical(y)\n",
    "    y = y.reshape(len(y), 2, 1)\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    Convolutional((1, 28, 28), 3, 5),\n",
    "    Sigmoid(),\n",
    "    Reshape((5, 26, 26), (5 * 26 * 26, 1)),\n",
    "    Dense(5 * 26 * 26, 100),\n",
    "    Sigmoid(),\n",
    "    Dense(100, 2),\n",
    "    Sigmoid()\n",
    "]\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    error = 0\n",
    "    for x, y in zip(x_train, y_train):\n",
    "        # Forward pass\n",
    "        output = x\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        # Calculate error\n",
    "        error += binary_cross_entropy(y, output)\n",
    "\n",
    "        # Backward pass\n",
    "        grad = binary_cross_entropy_prime(y, output)\n",
    "        \n",
    "        for layer in reversed(network):\n",
    "            grad = layer.backward(grad, learning_rate)\n",
    "    \n",
    "    error /= len(x_train)\n",
    "    print(f\"Epoch {epoch}, error: {error}\")\n",
    "\n",
    "correct = 0\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = x\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "        correct += 1 if np.argmax(output) == np.argmax(y) else 0\n",
    "\n",
    "print(f\"\\nNumber of correct predictions: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "Inspiration is taken from:\n",
    "- [The Independent Code, YouTube series on \"Convolutional Neural Network from Scratch\"](https://www.youtube.com/watch?v=Lakz2MoHy6o&ab_channel=TheIndependentCode)\n",
    "- [2D Convolution using Python & NumPy](https://medium.com/analytics-vidhya/2d-convolution-using-python-numpy-43442ff5f381)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe975d092d2ac7f937a42acc0742499e035d25a93da4137fa8ab0db54717d31f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
